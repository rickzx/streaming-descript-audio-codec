{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor([[[ 0.2391,  0.2759,  0.8915,  ...,  2.2475,  1.3730,  2.1704],\n",
      "         [ 0.5690,  0.3283,  0.4935,  ...,  0.8048,  0.7730,  0.4228],\n",
      "         [-2.4467,  0.4928, -3.4065,  ..., -1.7856, -1.5958, -0.1326],\n",
      "         ...,\n",
      "         [-2.7717, -0.6683, -1.0477,  ..., -0.8070, -1.1878, -0.1473],\n",
      "         [ 0.9553,  0.9808,  0.7493,  ...,  0.8611,  1.7939,  1.8238],\n",
      "         [ 0.8837,  0.9023,  1.0396,  ...,  1.5872,  0.4577,  1.1960]],\n",
      "\n",
      "        [[ 0.7317,  2.8882,  1.5236,  ...,  3.4878,  4.8952,  0.2520],\n",
      "         [ 0.4875,  0.8073,  0.5325,  ...,  0.0925, -0.2831,  0.4675],\n",
      "         [-1.6555, -2.3700, -6.8651,  ..., -0.8240, -1.1611,  0.9850],\n",
      "         ...,\n",
      "         [-2.3175,  0.3974, -1.1541,  ..., -2.0387, -2.5608, -2.1537],\n",
      "         [ 0.1785,  0.1062,  0.9603,  ...,  1.4923,  0.2642,  1.1658],\n",
      "         [ 0.6244,  0.9748,  0.4891,  ...,  1.1547,  2.0384,  0.3623]],\n",
      "\n",
      "        [[ 1.5471,  2.3142, -0.0321,  ...,  3.3907,  1.0151,  1.6523],\n",
      "         [ 0.4586,  0.5339,  0.6181,  ...,  0.4847,  0.4774,  0.2738],\n",
      "         [ 0.5580, -4.5806, -4.2872,  ..., -0.5131, -1.7315, -2.7107],\n",
      "         ...,\n",
      "         [-1.8273, -0.8208, -2.4517,  ..., -0.5116, -1.3388, -0.9865],\n",
      "         [ 1.8638,  1.4054,  0.8197,  ...,  0.4049,  1.1175,  1.8428],\n",
      "         [ 1.2679,  0.9172,  0.3428,  ...,  1.0127,  0.9199,  0.7257]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.4303,  0.9407,  2.3502,  ...,  3.3942,  0.6659,  2.2038],\n",
      "         [ 0.8776,  0.6746,  0.8299,  ...,  0.5432,  0.0854,  0.9620],\n",
      "         [-1.5673, -2.2818, -1.1140,  ..., -0.3446, -3.4646,  2.6042],\n",
      "         ...,\n",
      "         [-0.9756,  0.7678, -1.0456,  ..., -1.2936, -1.6903, -0.6169],\n",
      "         [ 0.3488,  0.2618,  0.7301,  ...,  1.3557,  0.4638,  1.2840],\n",
      "         [ 1.2235,  0.1695,  1.3875,  ...,  1.1901,  0.7750,  0.7964]],\n",
      "\n",
      "        [[-0.4298,  3.1034,  0.5441,  ..., -1.2649,  0.4618,  1.4332],\n",
      "         [ 0.7998,  0.5493,  0.3957,  ...,  0.2120,  0.3455,  0.5838],\n",
      "         [-6.2248, -3.5599,  2.3173,  ...,  0.8402, -2.2254, -5.6610],\n",
      "         ...,\n",
      "         [-1.3530, -1.3505, -0.2200,  ..., -4.3648, -1.3759, -1.9862],\n",
      "         [ 0.5831,  0.0352,  1.0047,  ...,  1.7583,  1.3935,  0.4970],\n",
      "         [ 0.1642,  0.5625,  0.8667,  ...,  1.3045,  0.6640,  0.9124]],\n",
      "\n",
      "        [[ 1.1173, -0.5701,  2.2973,  ...,  1.5074, -0.0987, -0.0401],\n",
      "         [ 0.1803,  0.1208,  0.0668,  ...,  0.4868,  1.0641,  0.4058],\n",
      "         [-0.4448, -3.3007, -3.5659,  ...,  1.1532, -2.6787, -2.4315],\n",
      "         ...,\n",
      "         [-0.8209, -2.2548, -2.2953,  ..., -1.5619, -0.6384, -0.8196],\n",
      "         [ 1.5494,  0.4939,  0.9105,  ...,  0.5522,  0.8653,  1.0681],\n",
      "         [ 1.4491,  0.6100,  1.2613,  ...,  1.6963,  0.3467,  1.1721]]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[[ 0.2391,  0.2759,  0.8915,  ...,  2.2475,  1.3730,  2.1704],\n",
      "         [ 0.5690,  0.3283,  0.4935,  ...,  0.8048,  0.7730,  0.4228],\n",
      "         [-2.4467,  0.4928, -3.4065,  ..., -1.7856, -1.5958, -0.1326],\n",
      "         ...,\n",
      "         [-2.7717, -0.6683, -1.0477,  ..., -0.8070, -1.1878, -0.1473],\n",
      "         [ 0.9553,  0.9808,  0.7493,  ...,  0.8611,  1.7939,  1.8238],\n",
      "         [ 0.8837,  0.9023,  1.0396,  ...,  1.5872,  0.4577,  1.1960]],\n",
      "\n",
      "        [[ 0.7317,  2.8882,  1.5236,  ...,  3.4878,  4.8952,  0.2520],\n",
      "         [ 0.4875,  0.8073,  0.5325,  ...,  0.0925, -0.2831,  0.4675],\n",
      "         [-1.6555, -2.3700, -6.8651,  ..., -0.8240, -1.1610,  0.9850],\n",
      "         ...,\n",
      "         [-2.3175,  0.3974, -1.1541,  ..., -2.0387, -2.5608, -2.1537],\n",
      "         [ 0.1785,  0.1062,  0.9603,  ...,  1.4923,  0.2642,  1.1658],\n",
      "         [ 0.6244,  0.9748,  0.4891,  ...,  1.1547,  2.0384,  0.3623]],\n",
      "\n",
      "        [[ 1.5471,  2.3142, -0.0321,  ...,  3.3907,  1.0151,  1.6523],\n",
      "         [ 0.4586,  0.5339,  0.6181,  ...,  0.4847,  0.4774,  0.2738],\n",
      "         [ 0.5580, -4.5806, -4.2872,  ..., -0.5131, -1.7315, -2.7107],\n",
      "         ...,\n",
      "         [-1.8273, -0.8208, -2.4517,  ..., -0.5116, -1.3388, -0.9865],\n",
      "         [ 1.8638,  1.4054,  0.8197,  ...,  0.4049,  1.1175,  1.8428],\n",
      "         [ 1.2679,  0.9172,  0.3428,  ...,  1.0127,  0.9199,  0.7257]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.4303,  0.9407,  2.3502,  ...,  3.3942,  0.6659,  2.2038],\n",
      "         [ 0.8776,  0.6746,  0.8299,  ...,  0.5432,  0.0854,  0.9620],\n",
      "         [-1.5673, -2.2818, -1.1140,  ..., -0.3446, -3.4646,  2.6042],\n",
      "         ...,\n",
      "         [-0.9756,  0.7678, -1.0456,  ..., -1.2936, -1.6903, -0.6169],\n",
      "         [ 0.3488,  0.2618,  0.7301,  ...,  1.3557,  0.4638,  1.2840],\n",
      "         [ 1.2235,  0.1695,  1.3875,  ...,  1.1901,  0.7750,  0.7964]],\n",
      "\n",
      "        [[-0.4298,  3.1034,  0.5441,  ..., -1.2649,  0.4618,  1.4332],\n",
      "         [ 0.7998,  0.5493,  0.3957,  ...,  0.2120,  0.3455,  0.5838],\n",
      "         [-6.2248, -3.5599,  2.3173,  ...,  0.8402, -2.2254, -5.6610],\n",
      "         ...,\n",
      "         [-1.3530, -1.3505, -0.2200,  ..., -4.3648, -1.3759, -1.9862],\n",
      "         [ 0.5831,  0.0352,  1.0047,  ...,  1.7583,  1.3935,  0.4970],\n",
      "         [ 0.1642,  0.5625,  0.8667,  ...,  1.3045,  0.6640,  0.9124]],\n",
      "\n",
      "        [[ 1.1173, -0.5701,  2.2973,  ...,  1.5074, -0.0987, -0.0401],\n",
      "         [ 0.1803,  0.1208,  0.0668,  ...,  0.4868,  1.0641,  0.4058],\n",
      "         [-0.4448, -3.3007, -3.5659,  ...,  1.1532, -2.6787, -2.4315],\n",
      "         ...,\n",
      "         [-0.8209, -2.2548, -2.2953,  ..., -1.5619, -0.6384, -0.8196],\n",
      "         [ 1.5494,  0.4939,  0.9105,  ...,  0.5522,  0.8653,  1.0681],\n",
      "         [ 1.4491,  0.6100,  1.2613,  ...,  1.6963,  0.3467,  1.1721]]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from torch.nn.utils import weight_norm\n",
    "\n",
    "import cached_conv as cc\n",
    "\n",
    "cc.use_cached_conv(True)\n",
    "\n",
    "\n",
    "def WNConv1d(*args, **kwargs):\n",
    "    return weight_norm(nn.Conv1d(*args, **kwargs))\n",
    "\n",
    "def WNConv1dCached(*args, **kwargs):\n",
    "    return weight_norm(cc.Conv1d(*args, **kwargs))\n",
    "\n",
    "wn_conv1d = WNConv1d(5, 10, 7, padding=3)\n",
    "wn_conv1d_cached = WNConv1dCached(5, 10, 7, padding=3)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "weight_g_data = torch.randn(10, 1, 1)\n",
    "weight_v_data = torch.randn(10, 5, 7)\n",
    "bias_data = torch.randn(10)\n",
    "wn_conv1d.weight_g.data = weight_g_data\n",
    "wn_conv1d.weight_v.data = weight_v_data\n",
    "wn_conv1d.bias.data = bias_data\n",
    "\n",
    "data = torch.randn(20, 5, 1000)\n",
    "out = wn_conv1d(data)\n",
    "\n",
    "wn_conv1d_cached.weight_g.data = weight_g_data\n",
    "wn_conv1d_cached.weight_v.data = weight_v_data\n",
    "wn_conv1d_cached.bias.data = bias_data\n",
    "\n",
    "out_cached = wn_conv1d_cached(data)\n",
    "\n",
    "print(torch.allclose(out[..., :-wn_conv1d_cached.cumulative_delay], out_cached[..., wn_conv1d_cached.cumulative_delay:], atol=1e-6))\n",
    "print(out[..., :-wn_conv1d_cached.cumulative_delay])\n",
    "print(out_cached[..., wn_conv1d_cached.cumulative_delay:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 6]) torch.Size([1, 1, 6])\n",
      "tensor([[[-0.8099, -0.9363,  0.2203, -0.2220, -1.2132,  0.0279]]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "tensor([[[-0.9363,  0.2203, -0.2220, -1.2132,  0.0279, -0.5633]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cfruan/miniconda3/envs/mlc-audio/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc.use_cached_conv(True)\n",
    "\n",
    "wn_conv1d = WNConv1d(1, 1, 3, padding=1)\n",
    "wn_conv1d_cached = WNConv1dCached(1, 1, 3, padding=cc.get_padding(3))\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "data = torch.randn(1, 1, 6)\n",
    "\n",
    "weight_g_data = torch.randn(1, 1, 1)\n",
    "weight_v_data = torch.randn(1, 1, 3)\n",
    "bias_data = torch.randn(1)\n",
    "wn_conv1d.weight_g.data = weight_g_data\n",
    "wn_conv1d.weight_v.data = weight_v_data\n",
    "wn_conv1d.bias.data = bias_data\n",
    "\n",
    "wn_conv1d_cached.weight_g.data = weight_g_data\n",
    "wn_conv1d_cached.weight_v.data = weight_v_data\n",
    "wn_conv1d_cached.bias.data = bias_data\n",
    "\n",
    "chunk_size = 3\n",
    "res = []\n",
    "for i in range(0, data.shape[-1], chunk_size):\n",
    "    # Slice along the sequence dimension and apply conv1d to each chunk\n",
    "    tok = wn_conv1d_cached(data[..., i:i + chunk_size])\n",
    "    res.append(tok)\n",
    "\n",
    "# Concatenate all the chunks along the sequence dimension\n",
    "chunked_output = torch.cat(res, dim=-1)\n",
    "\n",
    "# Direct (non-chunked) computation for comparison\n",
    "non_chunked_output = wn_conv1d(data)\n",
    "\n",
    "print(chunked_output.shape, non_chunked_output.shape)\n",
    "print(chunked_output)\n",
    "print(non_chunked_output)\n",
    "\n",
    "wn_conv1d_cached.cumulative_delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cfruan/miniconda3/envs/mlc-audio/lib/python3.11/site-packages/audiotools/ml/layers/base.py:172: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_dict = torch.load(location, \"cpu\")\n",
      "/Users/cfruan/miniconda3/envs/mlc-audio/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['encoder.block.0.bias', 'encoder.block.0.weight_g', 'encoder.block.0.weight_v', 'encoder.block.1.block.0.block.0.alpha', 'encoder.block.1.block.0.block.1.bias', 'encoder.block.1.block.0.block.1.weight_g', 'encoder.block.1.block.0.block.1.weight_v', 'encoder.block.1.block.0.block.2.alpha', 'encoder.block.1.block.0.block.3.bias', 'encoder.block.1.block.0.block.3.weight_g', 'encoder.block.1.block.0.block.3.weight_v', 'encoder.block.1.block.1.block.0.alpha', 'encoder.block.1.block.1.block.1.bias', 'encoder.block.1.block.1.block.1.weight_g', 'encoder.block.1.block.1.block.1.weight_v', 'encoder.block.1.block.1.block.2.alpha', 'encoder.block.1.block.1.block.3.bias', 'encoder.block.1.block.1.block.3.weight_g', 'encoder.block.1.block.1.block.3.weight_v', 'encoder.block.1.block.2.block.0.alpha', 'encoder.block.1.block.2.block.1.bias', 'encoder.block.1.block.2.block.1.weight_g', 'encoder.block.1.block.2.block.1.weight_v', 'encoder.block.1.block.2.block.2.alpha', 'encoder.block.1.block.2.block.3.bias', 'encoder.block.1.block.2.block.3.weight_g', 'encoder.block.1.block.2.block.3.weight_v', 'encoder.block.1.block.3.alpha', 'encoder.block.1.block.4.bias', 'encoder.block.1.block.4.weight_g', 'encoder.block.1.block.4.weight_v', 'encoder.block.2.block.0.block.0.alpha', 'encoder.block.2.block.0.block.1.bias', 'encoder.block.2.block.0.block.1.weight_g', 'encoder.block.2.block.0.block.1.weight_v', 'encoder.block.2.block.0.block.2.alpha', 'encoder.block.2.block.0.block.3.bias', 'encoder.block.2.block.0.block.3.weight_g', 'encoder.block.2.block.0.block.3.weight_v', 'encoder.block.2.block.1.block.0.alpha', 'encoder.block.2.block.1.block.1.bias', 'encoder.block.2.block.1.block.1.weight_g', 'encoder.block.2.block.1.block.1.weight_v', 'encoder.block.2.block.1.block.2.alpha', 'encoder.block.2.block.1.block.3.bias', 'encoder.block.2.block.1.block.3.weight_g', 'encoder.block.2.block.1.block.3.weight_v', 'encoder.block.2.block.2.block.0.alpha', 'encoder.block.2.block.2.block.1.bias', 'encoder.block.2.block.2.block.1.weight_g', 'encoder.block.2.block.2.block.1.weight_v', 'encoder.block.2.block.2.block.2.alpha', 'encoder.block.2.block.2.block.3.bias', 'encoder.block.2.block.2.block.3.weight_g', 'encoder.block.2.block.2.block.3.weight_v', 'encoder.block.2.block.3.alpha', 'encoder.block.2.block.4.bias', 'encoder.block.2.block.4.weight_g', 'encoder.block.2.block.4.weight_v', 'encoder.block.3.block.0.block.0.alpha', 'encoder.block.3.block.0.block.1.bias', 'encoder.block.3.block.0.block.1.weight_g', 'encoder.block.3.block.0.block.1.weight_v', 'encoder.block.3.block.0.block.2.alpha', 'encoder.block.3.block.0.block.3.bias', 'encoder.block.3.block.0.block.3.weight_g', 'encoder.block.3.block.0.block.3.weight_v', 'encoder.block.3.block.1.block.0.alpha', 'encoder.block.3.block.1.block.1.bias', 'encoder.block.3.block.1.block.1.weight_g', 'encoder.block.3.block.1.block.1.weight_v', 'encoder.block.3.block.1.block.2.alpha', 'encoder.block.3.block.1.block.3.bias', 'encoder.block.3.block.1.block.3.weight_g', 'encoder.block.3.block.1.block.3.weight_v', 'encoder.block.3.block.2.block.0.alpha', 'encoder.block.3.block.2.block.1.bias', 'encoder.block.3.block.2.block.1.weight_g', 'encoder.block.3.block.2.block.1.weight_v', 'encoder.block.3.block.2.block.2.alpha', 'encoder.block.3.block.2.block.3.bias', 'encoder.block.3.block.2.block.3.weight_g', 'encoder.block.3.block.2.block.3.weight_v', 'encoder.block.3.block.3.alpha', 'encoder.block.3.block.4.bias', 'encoder.block.3.block.4.weight_g', 'encoder.block.3.block.4.weight_v', 'encoder.block.4.block.0.block.0.alpha', 'encoder.block.4.block.0.block.1.bias', 'encoder.block.4.block.0.block.1.weight_g', 'encoder.block.4.block.0.block.1.weight_v', 'encoder.block.4.block.0.block.2.alpha', 'encoder.block.4.block.0.block.3.bias', 'encoder.block.4.block.0.block.3.weight_g', 'encoder.block.4.block.0.block.3.weight_v', 'encoder.block.4.block.1.block.0.alpha', 'encoder.block.4.block.1.block.1.bias', 'encoder.block.4.block.1.block.1.weight_g', 'encoder.block.4.block.1.block.1.weight_v', 'encoder.block.4.block.1.block.2.alpha', 'encoder.block.4.block.1.block.3.bias', 'encoder.block.4.block.1.block.3.weight_g', 'encoder.block.4.block.1.block.3.weight_v', 'encoder.block.4.block.2.block.0.alpha', 'encoder.block.4.block.2.block.1.bias', 'encoder.block.4.block.2.block.1.weight_g', 'encoder.block.4.block.2.block.1.weight_v', 'encoder.block.4.block.2.block.2.alpha', 'encoder.block.4.block.2.block.3.bias', 'encoder.block.4.block.2.block.3.weight_g', 'encoder.block.4.block.2.block.3.weight_v', 'encoder.block.4.block.3.alpha', 'encoder.block.4.block.4.bias', 'encoder.block.4.block.4.weight_g', 'encoder.block.4.block.4.weight_v', 'encoder.block.5.alpha', 'encoder.block.6.bias', 'encoder.block.6.weight_g', 'encoder.block.6.weight_v', 'quantizer.quantizers.0.in_proj.bias', 'quantizer.quantizers.0.in_proj.weight_g', 'quantizer.quantizers.0.in_proj.weight_v', 'quantizer.quantizers.0.out_proj.bias', 'quantizer.quantizers.0.out_proj.weight_g', 'quantizer.quantizers.0.out_proj.weight_v', 'quantizer.quantizers.0.codebook.weight', 'quantizer.quantizers.1.in_proj.bias', 'quantizer.quantizers.1.in_proj.weight_g', 'quantizer.quantizers.1.in_proj.weight_v', 'quantizer.quantizers.1.out_proj.bias', 'quantizer.quantizers.1.out_proj.weight_g', 'quantizer.quantizers.1.out_proj.weight_v', 'quantizer.quantizers.1.codebook.weight', 'quantizer.quantizers.2.in_proj.bias', 'quantizer.quantizers.2.in_proj.weight_g', 'quantizer.quantizers.2.in_proj.weight_v', 'quantizer.quantizers.2.out_proj.bias', 'quantizer.quantizers.2.out_proj.weight_g', 'quantizer.quantizers.2.out_proj.weight_v', 'quantizer.quantizers.2.codebook.weight', 'quantizer.quantizers.3.in_proj.bias', 'quantizer.quantizers.3.in_proj.weight_g', 'quantizer.quantizers.3.in_proj.weight_v', 'quantizer.quantizers.3.out_proj.bias', 'quantizer.quantizers.3.out_proj.weight_g', 'quantizer.quantizers.3.out_proj.weight_v', 'quantizer.quantizers.3.codebook.weight', 'quantizer.quantizers.4.in_proj.bias', 'quantizer.quantizers.4.in_proj.weight_g', 'quantizer.quantizers.4.in_proj.weight_v', 'quantizer.quantizers.4.out_proj.bias', 'quantizer.quantizers.4.out_proj.weight_g', 'quantizer.quantizers.4.out_proj.weight_v', 'quantizer.quantizers.4.codebook.weight', 'quantizer.quantizers.5.in_proj.bias', 'quantizer.quantizers.5.in_proj.weight_g', 'quantizer.quantizers.5.in_proj.weight_v', 'quantizer.quantizers.5.out_proj.bias', 'quantizer.quantizers.5.out_proj.weight_g', 'quantizer.quantizers.5.out_proj.weight_v', 'quantizer.quantizers.5.codebook.weight', 'quantizer.quantizers.6.in_proj.bias', 'quantizer.quantizers.6.in_proj.weight_g', 'quantizer.quantizers.6.in_proj.weight_v', 'quantizer.quantizers.6.out_proj.bias', 'quantizer.quantizers.6.out_proj.weight_g', 'quantizer.quantizers.6.out_proj.weight_v', 'quantizer.quantizers.6.codebook.weight', 'quantizer.quantizers.7.in_proj.bias', 'quantizer.quantizers.7.in_proj.weight_g', 'quantizer.quantizers.7.in_proj.weight_v', 'quantizer.quantizers.7.out_proj.bias', 'quantizer.quantizers.7.out_proj.weight_g', 'quantizer.quantizers.7.out_proj.weight_v', 'quantizer.quantizers.7.codebook.weight', 'quantizer.quantizers.8.in_proj.bias', 'quantizer.quantizers.8.in_proj.weight_g', 'quantizer.quantizers.8.in_proj.weight_v', 'quantizer.quantizers.8.out_proj.bias', 'quantizer.quantizers.8.out_proj.weight_g', 'quantizer.quantizers.8.out_proj.weight_v', 'quantizer.quantizers.8.codebook.weight', 'decoder.model.0.bias', 'decoder.model.0.weight_g', 'decoder.model.0.weight_v', 'decoder.model.1.block.0.alpha', 'decoder.model.1.block.1.bias', 'decoder.model.1.block.1.weight_g', 'decoder.model.1.block.1.weight_v', 'decoder.model.1.block.2.block.0.alpha', 'decoder.model.1.block.2.block.1.bias', 'decoder.model.1.block.2.block.1.weight_g', 'decoder.model.1.block.2.block.1.weight_v', 'decoder.model.1.block.2.block.2.alpha', 'decoder.model.1.block.2.block.3.bias', 'decoder.model.1.block.2.block.3.weight_g', 'decoder.model.1.block.2.block.3.weight_v', 'decoder.model.1.block.3.block.0.alpha', 'decoder.model.1.block.3.block.1.bias', 'decoder.model.1.block.3.block.1.weight_g', 'decoder.model.1.block.3.block.1.weight_v', 'decoder.model.1.block.3.block.2.alpha', 'decoder.model.1.block.3.block.3.bias', 'decoder.model.1.block.3.block.3.weight_g', 'decoder.model.1.block.3.block.3.weight_v', 'decoder.model.1.block.4.block.0.alpha', 'decoder.model.1.block.4.block.1.bias', 'decoder.model.1.block.4.block.1.weight_g', 'decoder.model.1.block.4.block.1.weight_v', 'decoder.model.1.block.4.block.2.alpha', 'decoder.model.1.block.4.block.3.bias', 'decoder.model.1.block.4.block.3.weight_g', 'decoder.model.1.block.4.block.3.weight_v', 'decoder.model.2.block.0.alpha', 'decoder.model.2.block.1.bias', 'decoder.model.2.block.1.weight_g', 'decoder.model.2.block.1.weight_v', 'decoder.model.2.block.2.block.0.alpha', 'decoder.model.2.block.2.block.1.bias', 'decoder.model.2.block.2.block.1.weight_g', 'decoder.model.2.block.2.block.1.weight_v', 'decoder.model.2.block.2.block.2.alpha', 'decoder.model.2.block.2.block.3.bias', 'decoder.model.2.block.2.block.3.weight_g', 'decoder.model.2.block.2.block.3.weight_v', 'decoder.model.2.block.3.block.0.alpha', 'decoder.model.2.block.3.block.1.bias', 'decoder.model.2.block.3.block.1.weight_g', 'decoder.model.2.block.3.block.1.weight_v', 'decoder.model.2.block.3.block.2.alpha', 'decoder.model.2.block.3.block.3.bias', 'decoder.model.2.block.3.block.3.weight_g', 'decoder.model.2.block.3.block.3.weight_v', 'decoder.model.2.block.4.block.0.alpha', 'decoder.model.2.block.4.block.1.bias', 'decoder.model.2.block.4.block.1.weight_g', 'decoder.model.2.block.4.block.1.weight_v', 'decoder.model.2.block.4.block.2.alpha', 'decoder.model.2.block.4.block.3.bias', 'decoder.model.2.block.4.block.3.weight_g', 'decoder.model.2.block.4.block.3.weight_v', 'decoder.model.3.block.0.alpha', 'decoder.model.3.block.1.bias', 'decoder.model.3.block.1.weight_g', 'decoder.model.3.block.1.weight_v', 'decoder.model.3.block.2.block.0.alpha', 'decoder.model.3.block.2.block.1.bias', 'decoder.model.3.block.2.block.1.weight_g', 'decoder.model.3.block.2.block.1.weight_v', 'decoder.model.3.block.2.block.2.alpha', 'decoder.model.3.block.2.block.3.bias', 'decoder.model.3.block.2.block.3.weight_g', 'decoder.model.3.block.2.block.3.weight_v', 'decoder.model.3.block.3.block.0.alpha', 'decoder.model.3.block.3.block.1.bias', 'decoder.model.3.block.3.block.1.weight_g', 'decoder.model.3.block.3.block.1.weight_v', 'decoder.model.3.block.3.block.2.alpha', 'decoder.model.3.block.3.block.3.bias', 'decoder.model.3.block.3.block.3.weight_g', 'decoder.model.3.block.3.block.3.weight_v', 'decoder.model.3.block.4.block.0.alpha', 'decoder.model.3.block.4.block.1.bias', 'decoder.model.3.block.4.block.1.weight_g', 'decoder.model.3.block.4.block.1.weight_v', 'decoder.model.3.block.4.block.2.alpha', 'decoder.model.3.block.4.block.3.bias', 'decoder.model.3.block.4.block.3.weight_g', 'decoder.model.3.block.4.block.3.weight_v', 'decoder.model.4.block.0.alpha', 'decoder.model.4.block.1.bias', 'decoder.model.4.block.1.weight_g', 'decoder.model.4.block.1.weight_v', 'decoder.model.4.block.2.block.0.alpha', 'decoder.model.4.block.2.block.1.bias', 'decoder.model.4.block.2.block.1.weight_g', 'decoder.model.4.block.2.block.1.weight_v', 'decoder.model.4.block.2.block.2.alpha', 'decoder.model.4.block.2.block.3.bias', 'decoder.model.4.block.2.block.3.weight_g', 'decoder.model.4.block.2.block.3.weight_v', 'decoder.model.4.block.3.block.0.alpha', 'decoder.model.4.block.3.block.1.bias', 'decoder.model.4.block.3.block.1.weight_g', 'decoder.model.4.block.3.block.1.weight_v', 'decoder.model.4.block.3.block.2.alpha', 'decoder.model.4.block.3.block.3.bias', 'decoder.model.4.block.3.block.3.weight_g', 'decoder.model.4.block.3.block.3.weight_v', 'decoder.model.4.block.4.block.0.alpha', 'decoder.model.4.block.4.block.1.bias', 'decoder.model.4.block.4.block.1.weight_g', 'decoder.model.4.block.4.block.1.weight_v', 'decoder.model.4.block.4.block.2.alpha', 'decoder.model.4.block.4.block.3.bias', 'decoder.model.4.block.4.block.3.weight_g', 'decoder.model.4.block.4.block.3.weight_v', 'decoder.model.5.alpha', 'decoder.model.6.bias', 'decoder.model.6.weight_g', 'decoder.model.6.weight_v'])\n",
      "8\n",
      "torch.Size([2, 1024, 79])\n",
      "tensor([[[-15.36168,  -8.15357,   7.60691,  ...,  11.73184, -33.48824,\n",
      "           -8.36922],\n",
      "         [ 33.78895,  24.53244,  21.50717,  ...,  64.08475, -29.52306,\n",
      "           17.79490],\n",
      "         [-31.27174,  11.67223,   2.85345,  ...,  -1.45382,  10.24431,\n",
      "           -3.98584],\n",
      "         ...,\n",
      "         [ -0.16428,  51.71983,  -3.36099,  ..., -24.26361, -45.69700,\n",
      "          -18.96716],\n",
      "         [ -6.10136, -22.82890,  -9.66237,  ...,  23.12833,  46.61592,\n",
      "            2.97680],\n",
      "         [  2.06728, -31.97044, -12.96737,  ...,  54.93013,  27.78410,\n",
      "           -5.34276]],\n",
      "\n",
      "        [[-23.44169,  -0.25626, -15.15949,  ...,  -5.51479, -19.25436,\n",
      "            7.06409],\n",
      "         [ 19.18370, 110.19602, -38.52904,  ..., -23.03546,  23.37997,\n",
      "          -35.61088],\n",
      "         [ 29.31331, -19.75456,  15.49293,  ...,  -7.95825, -14.31256,\n",
      "          -17.62057],\n",
      "         ...,\n",
      "         [  5.98049,   9.67873, -28.17511,  ...,  13.71754,  18.50896,\n",
      "           77.77760],\n",
      "         [-10.72248, -93.88680,  -0.51110,  ...,  -4.26531, -12.30583,\n",
      "            6.42867],\n",
      "         [-12.69298,  27.83443, -36.04440,  ...,  32.79217,  23.75504,\n",
      "          -79.16788]]], grad_fn=<SliceBackward0>)\n",
      "=============================\n",
      "=============================\n",
      "odict_keys(['encoder.block.0.bias', 'encoder.block.0.weight_g', 'encoder.block.0.weight_v', 'encoder.block.1.block.0.block.0.alpha', 'encoder.block.1.block.0.block.1.bias', 'encoder.block.1.block.0.block.1.weight_g', 'encoder.block.1.block.0.block.1.weight_v', 'encoder.block.1.block.0.block.2.alpha', 'encoder.block.1.block.0.block.3.bias', 'encoder.block.1.block.0.block.3.weight_g', 'encoder.block.1.block.0.block.3.weight_v', 'encoder.block.1.block.1.block.0.alpha', 'encoder.block.1.block.1.block.1.bias', 'encoder.block.1.block.1.block.1.weight_g', 'encoder.block.1.block.1.block.1.weight_v', 'encoder.block.1.block.1.block.2.alpha', 'encoder.block.1.block.1.block.3.bias', 'encoder.block.1.block.1.block.3.weight_g', 'encoder.block.1.block.1.block.3.weight_v', 'encoder.block.1.block.2.block.0.alpha', 'encoder.block.1.block.2.block.1.bias', 'encoder.block.1.block.2.block.1.weight_g', 'encoder.block.1.block.2.block.1.weight_v', 'encoder.block.1.block.2.block.2.alpha', 'encoder.block.1.block.2.block.3.bias', 'encoder.block.1.block.2.block.3.weight_g', 'encoder.block.1.block.2.block.3.weight_v', 'encoder.block.1.block.3.alpha', 'encoder.block.1.block.4.bias', 'encoder.block.1.block.4.weight_g', 'encoder.block.1.block.4.weight_v', 'encoder.block.2.block.0.block.0.alpha', 'encoder.block.2.block.0.block.1.bias', 'encoder.block.2.block.0.block.1.weight_g', 'encoder.block.2.block.0.block.1.weight_v', 'encoder.block.2.block.0.block.2.alpha', 'encoder.block.2.block.0.block.3.bias', 'encoder.block.2.block.0.block.3.weight_g', 'encoder.block.2.block.0.block.3.weight_v', 'encoder.block.2.block.1.block.0.alpha', 'encoder.block.2.block.1.block.1.bias', 'encoder.block.2.block.1.block.1.weight_g', 'encoder.block.2.block.1.block.1.weight_v', 'encoder.block.2.block.1.block.2.alpha', 'encoder.block.2.block.1.block.3.bias', 'encoder.block.2.block.1.block.3.weight_g', 'encoder.block.2.block.1.block.3.weight_v', 'encoder.block.2.block.2.block.0.alpha', 'encoder.block.2.block.2.block.1.bias', 'encoder.block.2.block.2.block.1.weight_g', 'encoder.block.2.block.2.block.1.weight_v', 'encoder.block.2.block.2.block.2.alpha', 'encoder.block.2.block.2.block.3.bias', 'encoder.block.2.block.2.block.3.weight_g', 'encoder.block.2.block.2.block.3.weight_v', 'encoder.block.2.block.3.alpha', 'encoder.block.2.block.4.bias', 'encoder.block.2.block.4.weight_g', 'encoder.block.2.block.4.weight_v', 'encoder.block.3.block.0.block.0.alpha', 'encoder.block.3.block.0.block.1.bias', 'encoder.block.3.block.0.block.1.weight_g', 'encoder.block.3.block.0.block.1.weight_v', 'encoder.block.3.block.0.block.2.alpha', 'encoder.block.3.block.0.block.3.bias', 'encoder.block.3.block.0.block.3.weight_g', 'encoder.block.3.block.0.block.3.weight_v', 'encoder.block.3.block.1.block.0.alpha', 'encoder.block.3.block.1.block.1.bias', 'encoder.block.3.block.1.block.1.weight_g', 'encoder.block.3.block.1.block.1.weight_v', 'encoder.block.3.block.1.block.2.alpha', 'encoder.block.3.block.1.block.3.bias', 'encoder.block.3.block.1.block.3.weight_g', 'encoder.block.3.block.1.block.3.weight_v', 'encoder.block.3.block.2.block.0.alpha', 'encoder.block.3.block.2.block.1.bias', 'encoder.block.3.block.2.block.1.weight_g', 'encoder.block.3.block.2.block.1.weight_v', 'encoder.block.3.block.2.block.2.alpha', 'encoder.block.3.block.2.block.3.bias', 'encoder.block.3.block.2.block.3.weight_g', 'encoder.block.3.block.2.block.3.weight_v', 'encoder.block.3.block.3.alpha', 'encoder.block.3.block.4.bias', 'encoder.block.3.block.4.weight_g', 'encoder.block.3.block.4.weight_v', 'encoder.block.4.block.0.block.0.alpha', 'encoder.block.4.block.0.block.1.bias', 'encoder.block.4.block.0.block.1.weight_g', 'encoder.block.4.block.0.block.1.weight_v', 'encoder.block.4.block.0.block.2.alpha', 'encoder.block.4.block.0.block.3.bias', 'encoder.block.4.block.0.block.3.weight_g', 'encoder.block.4.block.0.block.3.weight_v', 'encoder.block.4.block.1.block.0.alpha', 'encoder.block.4.block.1.block.1.bias', 'encoder.block.4.block.1.block.1.weight_g', 'encoder.block.4.block.1.block.1.weight_v', 'encoder.block.4.block.1.block.2.alpha', 'encoder.block.4.block.1.block.3.bias', 'encoder.block.4.block.1.block.3.weight_g', 'encoder.block.4.block.1.block.3.weight_v', 'encoder.block.4.block.2.block.0.alpha', 'encoder.block.4.block.2.block.1.bias', 'encoder.block.4.block.2.block.1.weight_g', 'encoder.block.4.block.2.block.1.weight_v', 'encoder.block.4.block.2.block.2.alpha', 'encoder.block.4.block.2.block.3.bias', 'encoder.block.4.block.2.block.3.weight_g', 'encoder.block.4.block.2.block.3.weight_v', 'encoder.block.4.block.3.alpha', 'encoder.block.4.block.4.bias', 'encoder.block.4.block.4.weight_g', 'encoder.block.4.block.4.weight_v', 'encoder.block.5.alpha', 'encoder.block.6.bias', 'encoder.block.6.weight_g', 'encoder.block.6.weight_v', 'quantizer.quantizers.0.in_proj.bias', 'quantizer.quantizers.0.in_proj.weight_g', 'quantizer.quantizers.0.in_proj.weight_v', 'quantizer.quantizers.0.out_proj.bias', 'quantizer.quantizers.0.out_proj.weight_g', 'quantizer.quantizers.0.out_proj.weight_v', 'quantizer.quantizers.0.codebook.weight', 'quantizer.quantizers.1.in_proj.bias', 'quantizer.quantizers.1.in_proj.weight_g', 'quantizer.quantizers.1.in_proj.weight_v', 'quantizer.quantizers.1.out_proj.bias', 'quantizer.quantizers.1.out_proj.weight_g', 'quantizer.quantizers.1.out_proj.weight_v', 'quantizer.quantizers.1.codebook.weight', 'quantizer.quantizers.2.in_proj.bias', 'quantizer.quantizers.2.in_proj.weight_g', 'quantizer.quantizers.2.in_proj.weight_v', 'quantizer.quantizers.2.out_proj.bias', 'quantizer.quantizers.2.out_proj.weight_g', 'quantizer.quantizers.2.out_proj.weight_v', 'quantizer.quantizers.2.codebook.weight', 'quantizer.quantizers.3.in_proj.bias', 'quantizer.quantizers.3.in_proj.weight_g', 'quantizer.quantizers.3.in_proj.weight_v', 'quantizer.quantizers.3.out_proj.bias', 'quantizer.quantizers.3.out_proj.weight_g', 'quantizer.quantizers.3.out_proj.weight_v', 'quantizer.quantizers.3.codebook.weight', 'quantizer.quantizers.4.in_proj.bias', 'quantizer.quantizers.4.in_proj.weight_g', 'quantizer.quantizers.4.in_proj.weight_v', 'quantizer.quantizers.4.out_proj.bias', 'quantizer.quantizers.4.out_proj.weight_g', 'quantizer.quantizers.4.out_proj.weight_v', 'quantizer.quantizers.4.codebook.weight', 'quantizer.quantizers.5.in_proj.bias', 'quantizer.quantizers.5.in_proj.weight_g', 'quantizer.quantizers.5.in_proj.weight_v', 'quantizer.quantizers.5.out_proj.bias', 'quantizer.quantizers.5.out_proj.weight_g', 'quantizer.quantizers.5.out_proj.weight_v', 'quantizer.quantizers.5.codebook.weight', 'quantizer.quantizers.6.in_proj.bias', 'quantizer.quantizers.6.in_proj.weight_g', 'quantizer.quantizers.6.in_proj.weight_v', 'quantizer.quantizers.6.out_proj.bias', 'quantizer.quantizers.6.out_proj.weight_g', 'quantizer.quantizers.6.out_proj.weight_v', 'quantizer.quantizers.6.codebook.weight', 'quantizer.quantizers.7.in_proj.bias', 'quantizer.quantizers.7.in_proj.weight_g', 'quantizer.quantizers.7.in_proj.weight_v', 'quantizer.quantizers.7.out_proj.bias', 'quantizer.quantizers.7.out_proj.weight_g', 'quantizer.quantizers.7.out_proj.weight_v', 'quantizer.quantizers.7.codebook.weight', 'quantizer.quantizers.8.in_proj.bias', 'quantizer.quantizers.8.in_proj.weight_g', 'quantizer.quantizers.8.in_proj.weight_v', 'quantizer.quantizers.8.out_proj.bias', 'quantizer.quantizers.8.out_proj.weight_g', 'quantizer.quantizers.8.out_proj.weight_v', 'quantizer.quantizers.8.codebook.weight', 'decoder.model.0.bias', 'decoder.model.0.weight_g', 'decoder.model.0.weight_v', 'decoder.model.1.block.0.alpha', 'decoder.model.1.block.1.bias', 'decoder.model.1.block.1.weight_g', 'decoder.model.1.block.1.weight_v', 'decoder.model.1.block.2.block.0.alpha', 'decoder.model.1.block.2.block.1.bias', 'decoder.model.1.block.2.block.1.weight_g', 'decoder.model.1.block.2.block.1.weight_v', 'decoder.model.1.block.2.block.2.alpha', 'decoder.model.1.block.2.block.3.bias', 'decoder.model.1.block.2.block.3.weight_g', 'decoder.model.1.block.2.block.3.weight_v', 'decoder.model.1.block.3.block.0.alpha', 'decoder.model.1.block.3.block.1.bias', 'decoder.model.1.block.3.block.1.weight_g', 'decoder.model.1.block.3.block.1.weight_v', 'decoder.model.1.block.3.block.2.alpha', 'decoder.model.1.block.3.block.3.bias', 'decoder.model.1.block.3.block.3.weight_g', 'decoder.model.1.block.3.block.3.weight_v', 'decoder.model.1.block.4.block.0.alpha', 'decoder.model.1.block.4.block.1.bias', 'decoder.model.1.block.4.block.1.weight_g', 'decoder.model.1.block.4.block.1.weight_v', 'decoder.model.1.block.4.block.2.alpha', 'decoder.model.1.block.4.block.3.bias', 'decoder.model.1.block.4.block.3.weight_g', 'decoder.model.1.block.4.block.3.weight_v', 'decoder.model.2.block.0.alpha', 'decoder.model.2.block.1.bias', 'decoder.model.2.block.1.weight_g', 'decoder.model.2.block.1.weight_v', 'decoder.model.2.block.2.block.0.alpha', 'decoder.model.2.block.2.block.1.bias', 'decoder.model.2.block.2.block.1.weight_g', 'decoder.model.2.block.2.block.1.weight_v', 'decoder.model.2.block.2.block.2.alpha', 'decoder.model.2.block.2.block.3.bias', 'decoder.model.2.block.2.block.3.weight_g', 'decoder.model.2.block.2.block.3.weight_v', 'decoder.model.2.block.3.block.0.alpha', 'decoder.model.2.block.3.block.1.bias', 'decoder.model.2.block.3.block.1.weight_g', 'decoder.model.2.block.3.block.1.weight_v', 'decoder.model.2.block.3.block.2.alpha', 'decoder.model.2.block.3.block.3.bias', 'decoder.model.2.block.3.block.3.weight_g', 'decoder.model.2.block.3.block.3.weight_v', 'decoder.model.2.block.4.block.0.alpha', 'decoder.model.2.block.4.block.1.bias', 'decoder.model.2.block.4.block.1.weight_g', 'decoder.model.2.block.4.block.1.weight_v', 'decoder.model.2.block.4.block.2.alpha', 'decoder.model.2.block.4.block.3.bias', 'decoder.model.2.block.4.block.3.weight_g', 'decoder.model.2.block.4.block.3.weight_v', 'decoder.model.3.block.0.alpha', 'decoder.model.3.block.1.bias', 'decoder.model.3.block.1.weight_g', 'decoder.model.3.block.1.weight_v', 'decoder.model.3.block.2.block.0.alpha', 'decoder.model.3.block.2.block.1.bias', 'decoder.model.3.block.2.block.1.weight_g', 'decoder.model.3.block.2.block.1.weight_v', 'decoder.model.3.block.2.block.2.alpha', 'decoder.model.3.block.2.block.3.bias', 'decoder.model.3.block.2.block.3.weight_g', 'decoder.model.3.block.2.block.3.weight_v', 'decoder.model.3.block.3.block.0.alpha', 'decoder.model.3.block.3.block.1.bias', 'decoder.model.3.block.3.block.1.weight_g', 'decoder.model.3.block.3.block.1.weight_v', 'decoder.model.3.block.3.block.2.alpha', 'decoder.model.3.block.3.block.3.bias', 'decoder.model.3.block.3.block.3.weight_g', 'decoder.model.3.block.3.block.3.weight_v', 'decoder.model.3.block.4.block.0.alpha', 'decoder.model.3.block.4.block.1.bias', 'decoder.model.3.block.4.block.1.weight_g', 'decoder.model.3.block.4.block.1.weight_v', 'decoder.model.3.block.4.block.2.alpha', 'decoder.model.3.block.4.block.3.bias', 'decoder.model.3.block.4.block.3.weight_g', 'decoder.model.3.block.4.block.3.weight_v', 'decoder.model.4.block.0.alpha', 'decoder.model.4.block.1.bias', 'decoder.model.4.block.1.weight_g', 'decoder.model.4.block.1.weight_v', 'decoder.model.4.block.2.block.0.alpha', 'decoder.model.4.block.2.block.1.bias', 'decoder.model.4.block.2.block.1.weight_g', 'decoder.model.4.block.2.block.1.weight_v', 'decoder.model.4.block.2.block.2.alpha', 'decoder.model.4.block.2.block.3.bias', 'decoder.model.4.block.2.block.3.weight_g', 'decoder.model.4.block.2.block.3.weight_v', 'decoder.model.4.block.3.block.0.alpha', 'decoder.model.4.block.3.block.1.bias', 'decoder.model.4.block.3.block.1.weight_g', 'decoder.model.4.block.3.block.1.weight_v', 'decoder.model.4.block.3.block.2.alpha', 'decoder.model.4.block.3.block.3.bias', 'decoder.model.4.block.3.block.3.weight_g', 'decoder.model.4.block.3.block.3.weight_v', 'decoder.model.4.block.4.block.0.alpha', 'decoder.model.4.block.4.block.1.bias', 'decoder.model.4.block.4.block.1.weight_g', 'decoder.model.4.block.4.block.1.weight_v', 'decoder.model.4.block.4.block.2.alpha', 'decoder.model.4.block.4.block.3.bias', 'decoder.model.4.block.4.block.3.weight_g', 'decoder.model.4.block.4.block.3.weight_v', 'decoder.model.5.alpha', 'decoder.model.6.bias', 'decoder.model.6.weight_g', 'decoder.model.6.weight_v'])\n",
      "0\n",
      "torch.Size([2, 1024, 79])\n",
      "tensor([[[-16.68392,  -6.52436,   7.93897,  ...,  11.73186, -33.48825,\n",
      "           -8.36923],\n",
      "         [ 29.19920,  20.29701,  20.12698,  ...,  64.08472, -29.52305,\n",
      "           17.79493],\n",
      "         [-20.86957,  11.20798,   1.87899,  ...,  -1.45382,  10.24425,\n",
      "           -3.98577],\n",
      "         ...,\n",
      "         [ -4.30367,  51.39360,  -4.48995,  ..., -24.26353, -45.69708,\n",
      "          -18.96714],\n",
      "         [  0.56174, -26.66197,  -8.74627,  ...,  23.12829,  46.61594,\n",
      "            2.97680],\n",
      "         [ 15.70459, -32.56799, -10.94031,  ...,  54.93015,  27.78411,\n",
      "           -5.34279]],\n",
      "\n",
      "        [[-19.03696,  -4.18412, -14.57894,  ...,  -5.51478, -19.25435,\n",
      "            7.06407],\n",
      "         [ 14.43217, 113.05013, -38.78094,  ..., -23.03548,  23.37998,\n",
      "          -35.61092],\n",
      "         [ 18.84206, -15.23293,  17.14990,  ...,  -7.95823, -14.31255,\n",
      "          -17.62059],\n",
      "         ...,\n",
      "         [  8.39992,  11.13354, -29.05371,  ...,  13.71753,  18.50898,\n",
      "           77.77760],\n",
      "         [ -3.90366, -98.63791,  -1.26744,  ...,  -4.26528, -12.30586,\n",
      "            6.42867],\n",
      "         [ -8.07170,  29.44076, -39.33736,  ...,  32.79211,  23.75504,\n",
      "          -79.16781]]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import dac\n",
    "\n",
    "# Monkey patching the DAC class to use cc.Conv1d instead of nn.Conv1d\n",
    "\n",
    "# Download a model\n",
    "model_path = dac.utils.download(model_type=\"44khz\")\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "\n",
    "dac.DAC.enable_streaming(True)\n",
    "model = dac.DAC.load(model_path).to(\"cpu\")\n",
    "delay = model.encoder_cumulative_delay\n",
    "\n",
    "torch.set_printoptions(precision=5, sci_mode=False)\n",
    "\n",
    "# set numpy random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Load audio signal file\n",
    "silence = np.random.randn(*(2, 1, 44100)).astype(np.float32)\n",
    "data = torch.tensor(silence).to(\"cpu\")\n",
    "\n",
    "out = model.encode(data)\n",
    "print(model.encoder_cumulative_delay)\n",
    "out = out[..., delay:]\n",
    "print(out.shape)\n",
    "print(out)\n",
    "\n",
    "print(\"=============================\")\n",
    "print(\"=============================\")\n",
    "\n",
    "\n",
    "dac.DAC.enable_streaming(False)\n",
    "model = dac.DAC.load(model_path).to(\"cpu\")\n",
    "\n",
    "# #print all model parameters\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(name, param.shape)\n",
    "\n",
    "# set numpy random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Load audio signal file\n",
    "silence = np.random.randn(*(2, 1, 44100)).astype(np.float32)\n",
    "data = torch.tensor(silence).to(\"cpu\")\n",
    "\n",
    "out = model.encode(data)\n",
    "out = out[..., :-delay]\n",
    "print(model.encoder_cumulative_delay)\n",
    "print(out.shape)\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlc-audio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
